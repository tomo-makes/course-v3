{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rossman_data_clean_ja.ipynb","version":"0.3.2","provenance":[]}},"cells":[{"metadata":{"id":"_bic3tupCgxa","colab_type":"code","colab":{}},"cell_type":"code","source":["%reload_ext autoreload\n","%autoreload 2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pkxD3MQkCgxc","colab_type":"code","colab":{}},"cell_type":"code","source":["from fastai.basics import *"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K2QPhOp3Cgxe","colab_type":"text"},"cell_type":"markdown","source":["# Rossmann"]},{"metadata":{"id":"lALjBoL5Cgxf","colab_type":"text"},"cell_type":"markdown","source":["\n","# ロスマン\n"]},{"metadata":{"id":"76oHOa3lCgxf","colab_type":"text"},"cell_type":"markdown","source":["## Data preparation / Feature engineering"]},{"metadata":{"id":"tyl79U9aCgxg","colab_type":"text"},"cell_type":"markdown","source":["\n","## データセットの準備 / 特徴量エンジニアリング\n"]},{"metadata":{"id":"3j-ZQDmKCgxh","colab_type":"text"},"cell_type":"markdown","source":["In addition to the provided data, we will be using external datasets put together by participants in the Kaggle competition. You can download all of them [here](http://files.fast.ai/part2/lesson14/rossmann.tgz). Then you shold untar them in the dirctory to which `PATH` is pointing below.\n","\n","For completeness, the implementation used to put them together is included below."]},{"metadata":{"id":"fUxvV0a1Cgxh","colab_type":"text"},"cell_type":"markdown","source":["\n","提供されたデータに加えて、Kaggleコンテストの参加者によってまとめられた外部データセットを使用します。 [ここから](http://files.fast.ai/part2/lesson14/rossmann.tgz)すべてダウンロードできます。それから下記の`PATH`の示すディレクトリにそれらを解凍します。 \n","\n","完全を期すために、それらをまとめるために使うコードを以下に挙げます。 \n"]},{"metadata":{"id":"FeLUc74ACgxi","colab_type":"code","colab":{},"outputId":"4a314d9b-3f47-46d5-a4e0-b0861305ab28"},"cell_type":"code","source":["PATH=Path('data/rossmann/')\n","table_names = ['train', 'store', 'store_states', 'state_names', 'googletrend', 'weather', 'test']\n","tables = [pd.read_csv(PATH/f'{fname}.csv', low_memory=False) for fname in table_names]\n","train, store, store_states, state_names, googletrend, weather, test = tables\n","len(train),len(test)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1017209, 41088)"]},"metadata":{"tags":[]},"execution_count":0}]},{"metadata":{"id":"Fv1fz3TUCgxm","colab_type":"text"},"cell_type":"markdown","source":["We turn state Holidays to booleans, to make them more convenient for modeling. We can do calculations on pandas fields using notation very similar (often identical) to numpy."]},{"metadata":{"id":"jnMs-PdwCgxm","colab_type":"text"},"cell_type":"markdown","source":["\n","私たちは州の祝日を、モデルで扱いやすくするよう、Boolean値に変換します。numpyと非常によく似た（しばしば同一の）表記法を使用して、pandas上で計算を行うことができます。 \n"]},{"metadata":{"id":"v_bjxoieCgxn","colab_type":"code","colab":{}},"cell_type":"code","source":["train.StateHoliday = train.StateHoliday!='0'\n","test.StateHoliday = test.StateHoliday!='0'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vo3snGKWCgxp","colab_type":"text"},"cell_type":"markdown","source":["`join_df` is a function for joining tables on specific fields. By default, we'll be doing a left outer join of `right` on the `left` argument using the given fields for each table.\n","\n","Pandas does joins using the `merge` method. The `suffixes` argument describes the naming convention for duplicate fields. We've elected to leave the duplicate field names on the left untouched, and append a \"\\_y\" to those on the right."]},{"metadata":{"id":"o_QZq7CLCgxp","colab_type":"text"},"cell_type":"markdown","source":["\n"," `join_df`は特定のフィールドのテーブルを結合するための関数です。デフォルトでは、各テーブルに指定されたフィールドを使用して、 `left`引数で`right`から左外部結合を実行します。 \n","\n"," Pandasは`merge`メソッドを使って結合します。 `suffixes`引数は、重複フィールドの命名規則を指定します。左側の重複するフィールド名はそのまま残し、右側のフィールドには \"_y\"を追加することにしました。 \n"]},{"metadata":{"id":"gGPQXzdeCgxr","colab_type":"code","colab":{}},"cell_type":"code","source":["def join_df(left, right, left_on, right_on=None, suffix='_y'):\n","    if right_on is None: right_on = left_on\n","    return left.merge(right, how='left', left_on=left_on, right_on=right_on, \n","                      suffixes=(\"\", suffix))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7qGJTFV4Cgxt","colab_type":"text"},"cell_type":"markdown","source":["Join weather/state names."]},{"metadata":{"id":"vSl3Pu-HCgxt","colab_type":"text"},"cell_type":"markdown","source":["\n","天気/州の名前を結合します。 \n"]},{"metadata":{"id":"KITUlA1hCgxu","colab_type":"code","colab":{}},"cell_type":"code","source":["weather = join_df(weather, state_names, \"file\", \"StateName\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3b2l9BBwCgxx","colab_type":"text"},"cell_type":"markdown","source":["In pandas you can add new columns to a dataframe by simply defining it. We'll do this for googletrends by extracting dates and state names from the given data and adding those columns.\n","\n","We're also going to replace all instances of state name 'NI' to match the usage in the rest of the data: 'HB,NI'. This is a good opportunity to highlight pandas indexing. We can use `.loc[rows, cols]` to select a list of rows and a list of columns from the dataframe. In this case, we're selecting rows w/ statename 'NI' by using a boolean list `googletrend.State=='NI'` and selecting \"State\"."]},{"metadata":{"id":"Rt1zVXFBCgxx","colab_type":"text"},"cell_type":"markdown","source":["\n","pandasでは、データフレームを定義するだけで新しい列をデータフレームに追加できます。指定されたデータから日付と州名を抽出し、それらの列を追加することによって、Google Trendsに対してこれを行います。 \n","\n","また、状態名 &#39;NI&#39;のすべてのインスタンスを、残りのデータ &#39;HB、NI&#39;の使用方法に合わせて置き換えます。これはpandasのindexに光をあてる良い機会です。 `.loc[rows, cols]`から行のリストと列のリストを選択するために`.loc[rows, cols]`を使うことができます。この例では、Boolean値リスト`googletrend.State=='NI'`を使用して \"State\"を選択することで、状態名が`googletrend.State=='NI'`を選択しています。 \n"]},{"metadata":{"id":"qsSrfPwXCgxx","colab_type":"code","colab":{}},"cell_type":"code","source":["googletrend['Date'] = googletrend.week.str.split(' - ', expand=True)[0]\n","googletrend['State'] = googletrend.file.str.split('_', expand=True)[2]\n","googletrend.loc[googletrend.State=='NI', \"State\"] = 'HB,NI'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f6TBX1URCgx0","colab_type":"text"},"cell_type":"markdown","source":["The following extracts particular date fields from a complete datetime for the purpose of constructing categoricals.\n","\n","You should *always* consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can't capture any trend/cyclical behavior as a function of time at any of these granularities. We'll add to every table with a date field."]},{"metadata":{"id":"dTKTue8ECgx1","colab_type":"text"},"cell_type":"markdown","source":["\n","以下は、カテゴリカル変数を構築する目的で、日時型(datetime)から特定の日付フィールドを抽出します。 \n","\n","日時を扱うときは、この特徴抽出ステップを*常に*考慮する必要があります。日時型(datetime)から、これらの追加フィールドに拡張しなければ、これらどの粒度においても、時間の関数としてトレンド/周期的な振る舞いを捉えることはできません。すべてのテーブルに日付フィールドを追加します。 \n"]},{"metadata":{"id":"Q44iRPcLCgx1","colab_type":"code","colab":{}},"cell_type":"code","source":["def add_datepart(df, fldname, drop=True, time=False):\n","    \"Helper function that adds columns relevant to a date.\"\n","    fld = df[fldname]\n","    fld_dtype = fld.dtype\n","    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n","        fld_dtype = np.datetime64\n","\n","    if not np.issubdtype(fld_dtype, np.datetime64):\n","        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n","    targ_pre = re.sub('[Dd]ate$', '', fldname)\n","    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n","            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n","    if time: attr = attr + ['Hour', 'Minute', 'Second']\n","    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n","    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n","    if drop: df.drop(fldname, axis=1, inplace=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sjfrizTQCgx2","colab_type":"code","colab":{}},"cell_type":"code","source":["add_datepart(weather, \"Date\", drop=False)\n","add_datepart(googletrend, \"Date\", drop=False)\n","add_datepart(train, \"Date\", drop=False)\n","add_datepart(test, \"Date\", drop=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0FzEjV32Cgx4","colab_type":"text"},"cell_type":"markdown","source":["The Google trends data has a special category for the whole of the Germany - we'll pull that out so we can use it explicitly."]},{"metadata":{"id":"yMCpJrhGCgx5","colab_type":"text"},"cell_type":"markdown","source":["\n"," Google Trendsデータには、ドイツ全体のための特別なカテゴリがあります。明示的に使用できるように、それを取り出します。 \n"]},{"metadata":{"id":"FINjBLIiCgx6","colab_type":"code","colab":{}},"cell_type":"code","source":["trend_de = googletrend[googletrend.file == 'Rossmann_DE']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eTIeioyBCgx8","colab_type":"text"},"cell_type":"markdown","source":["Now we can outer join all of our data into a single dataframe. Recall that in outer joins everytime a value in the joining field on the left table does not have a corresponding value on the right table, the corresponding row in the new table has Null values for all right table fields. One way to check that all records are consistent and complete is to check for Null values post-join, as we do here.\n","\n","*Aside*: Why note just do an inner join?\n","If you are assuming that all records are complete and match on the field you desire, an inner join will do the same thing as an outer join. However, in the event you are wrong or a mistake is made, an outer join followed by a null-check will catch it. (Comparing before/after # of rows for inner join is equivalent, but requires keeping track of before/after row #'s. Outer join is easier.)"]},{"metadata":{"id":"6o72GQyyCgx-","colab_type":"text"},"cell_type":"markdown","source":["\n","これで、すべてのデータを単一のデータフレームに外部結合できます。左側のテーブルの結合フィールドの値が右側のテーブルの対応する値を持たないたびに、外部結合で新しいテーブルの対応する行がすべての右側のテーブルフィールドに対してNULL値を持つことを思い出してください。すべてのレコードが一貫していて完全であることを確認する1つの方法は、ここで行うように、結合後にNULL値を確認することです。 \n","\n"," *余談* ：なぜ内部結合をするだけなのですか？ \n","\n","すべてのレコードが完成し、目的のフィールドに一致すると想定している場合、内部結合は外部結合と同じことを行います。しかし、あなたが間違っていたり、間違いがあったりした場合には、外部結合の後にnullチェックが続くでしょう。 （内部結合の前後の行数を比較することは同等ですが、行の前後の行を追跡する必要があります。外部結合の方が簡単です。） \n"]},{"metadata":{"id":"1LNNpHDZCgx-","colab_type":"code","colab":{},"outputId":"3ff6b3e9-ed9c-4887-8f4e-3ffca981accc"},"cell_type":"code","source":["store = join_df(store, store_states, \"Store\")\n","len(store[store.State.isnull()])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":0}]},{"metadata":{"id":"lb8phmM6CgyB","colab_type":"code","colab":{},"outputId":"c7db863d-e6db-47af-afca-cd9d174dd016"},"cell_type":"code","source":["joined = join_df(train, store, \"Store\")\n","joined_test = join_df(test, store, \"Store\")\n","len(joined[joined.StoreType.isnull()]),len(joined_test[joined_test.StoreType.isnull()])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0, 0)"]},"metadata":{"tags":[]},"execution_count":0}]},{"metadata":{"id":"Jv9nhtn_CgyE","colab_type":"code","colab":{},"outputId":"2e017774-d1eb-43a9-b175-62ba26217763"},"cell_type":"code","source":["joined = join_df(joined, googletrend, [\"State\",\"Year\", \"Week\"])\n","joined_test = join_df(joined_test, googletrend, [\"State\",\"Year\", \"Week\"])\n","len(joined[joined.trend.isnull()]),len(joined_test[joined_test.trend.isnull()])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0, 0)"]},"metadata":{"tags":[]},"execution_count":0}]},{"metadata":{"id":"5mYBZa-XCgyI","colab_type":"code","colab":{},"outputId":"95ec6b23-621b-4e43-9bb5-9df970367dcf"},"cell_type":"code","source":["joined = joined.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\n","joined_test = joined_test.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\n","len(joined[joined.trend_DE.isnull()]),len(joined_test[joined_test.trend_DE.isnull()])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0, 0)"]},"metadata":{"tags":[]},"execution_count":0}]},{"metadata":{"id":"5i7BGnRbCgyM","colab_type":"code","colab":{},"outputId":"8e802ec3-4b55-4bfe-de59-656d17a00e94"},"cell_type":"code","source":["joined = join_df(joined, weather, [\"State\",\"Date\"])\n","joined_test = join_df(joined_test, weather, [\"State\",\"Date\"])\n","len(joined[joined.Mean_TemperatureC.isnull()]),len(joined_test[joined_test.Mean_TemperatureC.isnull()])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0, 0)"]},"metadata":{"tags":[]},"execution_count":0}]},{"metadata":{"id":"U0tc1XQICgyP","colab_type":"code","colab":{}},"cell_type":"code","source":["for df in (joined, joined_test):\n","    for c in df.columns:\n","        if c.endswith('_y'):\n","            if c in df.columns: df.drop(c, inplace=True, axis=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nBBUsTrHCgyR","colab_type":"text"},"cell_type":"markdown","source":["Next we'll fill in missing values to avoid complications with `NA`'s. `NA` (not available) is how Pandas indicates missing values; many models have problems when missing values are present, so it's always important to think about how to deal with them. In these cases, we are picking an arbitrary *signal value* that doesn't otherwise appear in the data."]},{"metadata":{"id":"HBQ7TRUbCgyS","colab_type":"text"},"cell_type":"markdown","source":["\n","次に、 `NA`での混乱を避けるために、欠損値を入力します。 `NA` （Not Available = 値が存在しない）は、Pandasが欠損値を示す方法です。欠損値が存在する場合、多くのモデルに問題があるため、それらをどのように処理するかを考えることは常に重要です。このような場合、他の方法ではデータに現れない任意の*シグナル値*を選びます。 \n"]},{"metadata":{"id":"6zCIaxjmCgyT","colab_type":"code","colab":{}},"cell_type":"code","source":["for df in (joined,joined_test):\n","    df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32)\n","    df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32)\n","    df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32)\n","    df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0VG_ptDVCgyU","colab_type":"text"},"cell_type":"markdown","source":["Next we'll extract features \"CompetitionOpenSince\" and \"CompetitionDaysOpen\". Note the use of `apply()` in mapping a function across dataframe values."]},{"metadata":{"id":"B_GIyWjRCgyU","colab_type":"text"},"cell_type":"markdown","source":["\n","次に、特徴量 \"CompetitionOpenSince\"と \"CompetitionDaysOpen\"を抽出します。データフレーム値を越えて関数をマッピングする際の`apply()`使用に注意してください。 \n"]},{"metadata":{"id":"x7ZCVCyxCgyV","colab_type":"code","colab":{}},"cell_type":"code","source":["for df in (joined,joined_test):\n","    df[\"CompetitionOpenSince\"] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear, \n","                                                     month=df.CompetitionOpenSinceMonth, day=15))\n","    df[\"CompetitionDaysOpen\"] = df.Date.subtract(df.CompetitionOpenSince).dt.days"],"execution_count":0,"outputs":[]},{"metadata":{"id":"07x95b2YCgyX","colab_type":"text"},"cell_type":"markdown","source":["We'll replace some erroneous / outlying data."]},{"metadata":{"id":"Qy7PU5-cCgyY","colab_type":"text"},"cell_type":"markdown","source":["\n","誤った/不適切なデータを置き換えます。 \n"]},{"metadata":{"id":"LavYLAkYCgyY","colab_type":"code","colab":{}},"cell_type":"code","source":["for df in (joined,joined_test):\n","    df.loc[df.CompetitionDaysOpen<0, \"CompetitionDaysOpen\"] = 0\n","    df.loc[df.CompetitionOpenSinceYear<1990, \"CompetitionDaysOpen\"] = 0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A-Ifi6G1Cgya","colab_type":"text"},"cell_type":"markdown","source":["We add \"CompetitionMonthsOpen\" field, limiting the maximum to 2 years to limit number of unique categories."]},{"metadata":{"id":"GhZKZGOVCgyb","colab_type":"text"},"cell_type":"markdown","source":["\n"," 「CompetitionMonthsOpen」フィールドを追加して、最大2年間を一意のカテゴリの数を制限するために制限します。 \n"]},{"metadata":{"id":"VwzGvPDvCgyc","colab_type":"code","colab":{},"outputId":"49a3c863-6d33-479e-ee8c-1e2b63778db9"},"cell_type":"code","source":["for df in (joined,joined_test):\n","    df[\"CompetitionMonthsOpen\"] = df[\"CompetitionDaysOpen\"]//30\n","    df.loc[df.CompetitionMonthsOpen>24, \"CompetitionMonthsOpen\"] = 24\n","joined.CompetitionMonthsOpen.unique()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([24,  3, 19,  9,  0, 16, 17,  7, 15, 22, 11, 13,  2, 23, 12,  4, 10,\n","        1, 14, 20,  8, 18,  6, 21,  5])"]},"metadata":{"tags":[]},"execution_count":0}]},{"metadata":{"id":"cU9EP_G4Cgyg","colab_type":"text"},"cell_type":"markdown","source":["Same process for Promo dates. You may need to install the `isoweek` package first."]},{"metadata":{"id":"UGQ4Sf1rCgyg","colab_type":"text"},"cell_type":"markdown","source":["\n","プロモの日付についても同じプロセスです。最初に`isoweek`パッケージをインストールする必要があるかもしれません。 \n"]},{"metadata":{"id":"hFNftlMECgyg","colab_type":"code","colab":{}},"cell_type":"code","source":["# If needed, uncomment:\n","# ! pip install isoweek"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DyxYtTLYCgyk","colab_type":"code","colab":{}},"cell_type":"code","source":["from isoweek import Week\n","for df in (joined,joined_test):\n","    df[\"Promo2Since\"] = pd.to_datetime(df.apply(lambda x: Week(\n","        x.Promo2SinceYear, x.Promo2SinceWeek).monday(), axis=1).astype(pd.datetime))\n","    df[\"Promo2Days\"] = df.Date.subtract(df[\"Promo2Since\"]).dt.days"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PjSi-eA6Cgyn","colab_type":"code","colab":{}},"cell_type":"code","source":["for df in (joined,joined_test):\n","    df.loc[df.Promo2Days<0, \"Promo2Days\"] = 0\n","    df.loc[df.Promo2SinceYear<1990, \"Promo2Days\"] = 0\n","    df[\"Promo2Weeks\"] = df[\"Promo2Days\"]//7\n","    df.loc[df.Promo2Weeks<0, \"Promo2Weeks\"] = 0\n","    df.loc[df.Promo2Weeks>25, \"Promo2Weeks\"] = 25\n","    df.Promo2Weeks.unique()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-3lfHmVuCgyp","colab_type":"code","colab":{}},"cell_type":"code","source":["joined.to_pickle(PATH/'joined')\n","joined_test.to_pickle(PATH/'joined_test')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qq-6x0MdCgyt","colab_type":"text"},"cell_type":"markdown","source":["## Durations"]},{"metadata":{"id":"VLIBTPBBCgyt","colab_type":"text"},"cell_type":"markdown","source":["\n","## 期間\n"]},{"metadata":{"id":"a9zjXmpYCgyu","colab_type":"text"},"cell_type":"markdown","source":["It is common when working with time series data to extract data that explains relationships across rows as opposed to columns, e.g.:\n","* Running averages\n","* Time until next event\n","* Time since last event\n","\n","This is often difficult to do with most table manipulation frameworks, since they are designed to work with relationships across columns. As such, we've created a class to handle this type of data.\n","\n","We'll define a function `get_elapsed` for cumulative counting across a sorted dataframe. Given a particular field `fld` to monitor, this function will start tracking time since the last occurrence of that field. When the field is seen again, the counter is set to zero.\n","\n","Upon initialization, this will result in datetime na's until the field is encountered. This is reset every time a new store is seen. We'll see how to use this shortly."]},{"metadata":{"id":"hqXLXesvCgyv","colab_type":"text"},"cell_type":"markdown","source":["\n","時系列データを扱うときには、列ではなく行にまたがる関係を説明するデータを抽出するのが一般的です。例えば、 \n","- 移動平均\n","- 次のイベントまでの時間\n","- 最後のイベントからの経過時間\n","\n","ほとんどのテーブル操作フレームワークは、列をまたいで関係を処理するように設計されているため、これはほとんどの場合困難です。そのため、この種のデータを処理するためのクラスを作成しました。 \n","\n","ソートされたデータフレーム全体で累積カウントするための関数`get_elapsed`を定義します。監視用のフィールド`fld`を使い、この関数はそのフィールドの最後に出現してからの時間の追跡を開始します。フィールドが再び出現すると、カウンタはゼロに設定されます。 \n","\n","初期化時には、これはフィールドに遭遇するまでdatetime NAになります。これは新しいストアが見つかるたびにリセットされます。すぐにこの使い方が出てきます。\n"]},{"metadata":{"id":"wabN6ERYCgyv","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_elapsed(fld, pre):\n","    day1 = np.timedelta64(1, 'D')\n","    last_date = np.datetime64()\n","    last_store = 0\n","    res = []\n","\n","    for s,v,d in zip(df.Store.values,df[fld].values, df.Date.values):\n","        if s != last_store:\n","            last_date = np.datetime64()\n","            last_store = s\n","        if v: last_date = d\n","        res.append(((d-last_date).astype('timedelta64[D]') / day1))\n","    df[pre+fld] = res"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tRxAz0uLCgyy","colab_type":"text"},"cell_type":"markdown","source":["We'll be applying this to a subset of columns:"]},{"metadata":{"id":"RBHrrsRSCgyz","colab_type":"text"},"cell_type":"markdown","source":["\n","これを列のサブセットに適用します。 \n"]},{"metadata":{"id":"hMH-_UU7Cgyz","colab_type":"code","colab":{}},"cell_type":"code","source":["columns = [\"Date\", \"Store\", \"Promo\", \"StateHoliday\", \"SchoolHoliday\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BgRpacYDCgy5","colab_type":"code","colab":{}},"cell_type":"code","source":["#df = train[columns]\n","df = train[columns].append(test[columns])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YkmSAS3TCgzE","colab_type":"text"},"cell_type":"markdown","source":["Let's walk through an example.\n","\n","Say we're looking at School Holiday. We'll first sort by Store, then Date, and then call `add_elapsed('SchoolHoliday', 'After')`:\n","This will apply to each row with School Holiday:\n","* A applied to every row of the dataframe in order of store and date\n","* Will add to the dataframe the days since seeing a School Holiday\n","* If we sort in the other direction, this will count the days until another holiday."]},{"metadata":{"id":"4AoyPWy1CgzE","colab_type":"text"},"cell_type":"markdown","source":["\n","例を見てみましょう。 \n","\n"," School Holidayを見ているとしましょう。最初にStoreでソートし、次にDateでソートし、次に`add_elapsed('SchoolHoliday', 'After')`を呼び出します。 \n","\n","これは、School Holidayの各行に適用されます。 \n","- 格納および日付順にデータフレームのすべての行に適用されます\n","- 学校の休日を見てからの日数がデータフレームに追加されます\n","- 別の方向にソートすると、これは別の休日までの日数を数えます\n"]},{"metadata":{"id":"dhWQIAHtCgzF","colab_type":"code","colab":{}},"cell_type":"code","source":["fld = 'SchoolHoliday'\n","df = df.sort_values(['Store', 'Date'])\n","get_elapsed(fld, 'After')\n","df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n","get_elapsed(fld, 'Before')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"osn7DGjaCgzH","colab_type":"text"},"cell_type":"markdown","source":["We'll do this for two more fields."]},{"metadata":{"id":"Dco6RpwdCgzH","colab_type":"text"},"cell_type":"markdown","source":["\n","もう2つのフィールドに対してこれを行います。 \n"]},{"metadata":{"id":"QDTOX-P-CgzH","colab_type":"code","colab":{}},"cell_type":"code","source":["fld = 'StateHoliday'\n","df = df.sort_values(['Store', 'Date'])\n","get_elapsed(fld, 'After')\n","df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n","get_elapsed(fld, 'Before')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Z_AHCQTcCgzJ","colab_type":"code","colab":{}},"cell_type":"code","source":["fld = 'Promo'\n","df = df.sort_values(['Store', 'Date'])\n","get_elapsed(fld, 'After')\n","df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n","get_elapsed(fld, 'Before')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TGABm_NsCgzL","colab_type":"text"},"cell_type":"markdown","source":["We're going to set the active index to Date."]},{"metadata":{"id":"2RU1JMarCgzM","colab_type":"text"},"cell_type":"markdown","source":["\n","アクティブインデックスをDateに設定します。 \n"]},{"metadata":{"id":"AAGueD7hCgzM","colab_type":"code","colab":{}},"cell_type":"code","source":["df = df.set_index(\"Date\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uuxukmdRCgzN","colab_type":"text"},"cell_type":"markdown","source":["Then set null values from elapsed field calculations to 0."]},{"metadata":{"id":"NgdPhWrRCgzO","colab_type":"text"},"cell_type":"markdown","source":["\n","その後、経過フィールドの計算からのNULL値を0に設定します。 \n"]},{"metadata":{"id":"NiZhqN28CgzO","colab_type":"code","colab":{}},"cell_type":"code","source":["columns = ['SchoolHoliday', 'StateHoliday', 'Promo']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"otENlmpzCgzP","colab_type":"code","colab":{}},"cell_type":"code","source":["for o in ['Before', 'After']:\n","    for p in columns:\n","        a = o+p\n","        df[a] = df[a].fillna(0).astype(int)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XwrlmUIrCgzR","colab_type":"text"},"cell_type":"markdown","source":["Next we'll demonstrate window functions in pandas to calculate rolling quantities.\n","\n","Here we're sorting by date (`sort_index()`) and counting the number of events of interest (`sum()`) defined in `columns` in the following week (`rolling()`), grouped by Store (`groupby()`). We do the same in the opposite direction."]},{"metadata":{"id":"Hif4EVGcCgzS","colab_type":"text"},"cell_type":"markdown","source":["\n","次に、ローリング計算をするためのウィンドウ関数をpandasで説明します。 \n","\n","ここでは、日付によるソート（`sort_index()`)を行い、と関心のあるイベント数のカウント（ `sum()`) を行い、それが`columns` で次の週に定義され（ `rolling()`) 、Storeによってグループ化（ `groupby()`)されます。反対方向にも同じことをします。 \n"]},{"metadata":{"id":"kK_ZOl00CgzS","colab_type":"code","colab":{}},"cell_type":"code","source":["bwd = df[['Store']+columns].sort_index().groupby(\"Store\").rolling(7, min_periods=1).sum()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1dVsvZXzCgzU","colab_type":"code","colab":{}},"cell_type":"code","source":["fwd = df[['Store']+columns].sort_index(ascending=False\n","                                      ).groupby(\"Store\").rolling(7, min_periods=1).sum()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"y2ymPtw7CgzV","colab_type":"text"},"cell_type":"markdown","source":["Next we want to drop the Store indices grouped together in the window function.\n","\n","Often in pandas, there is an option to do this in place. This is time and memory efficient when working with large datasets."]},{"metadata":{"id":"PcAAMGWECgzX","colab_type":"text"},"cell_type":"markdown","source":["\n","次に、ウィンドウ関数にまとめられたStoreインデックスを削除します。 \n","\n","多くの場合、pandasでは、これを実行するためのオプションがあります。大規模なデータセットを扱う場合、これは時間とメモリの観点で効率的です。\n"]},{"metadata":{"id":"fEgd7zqVCgzX","colab_type":"code","colab":{}},"cell_type":"code","source":["bwd.drop('Store',1,inplace=True)\n","bwd.reset_index(inplace=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"61N4aYEDCgzY","colab_type":"code","colab":{}},"cell_type":"code","source":["fwd.drop('Store',1,inplace=True)\n","fwd.reset_index(inplace=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7hkpxX17Cgza","colab_type":"code","colab":{}},"cell_type":"code","source":["df.reset_index(inplace=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"okIBaMtlCgzc","colab_type":"text"},"cell_type":"markdown","source":["Now we'll merge these values onto the df."]},{"metadata":{"id":"EJD0I1dUCgzc","colab_type":"text"},"cell_type":"markdown","source":["\n","それでは、これらの値をdfにマージします。 \n"]},{"metadata":{"id":"tqPaT1qpCgzc","colab_type":"code","colab":{}},"cell_type":"code","source":["df = df.merge(bwd, 'left', ['Date', 'Store'], suffixes=['', '_bw'])\n","df = df.merge(fwd, 'left', ['Date', 'Store'], suffixes=['', '_fw'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y489TLMoCgze","colab_type":"code","colab":{}},"cell_type":"code","source":["df.drop(columns,1,inplace=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yCSUMOU7Cgzh","colab_type":"code","colab":{},"outputId":"4307ef29-787f-4348-dff0-1e8319fad01c"},"cell_type":"code","source":["df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>Store</th>\n","      <th>AfterSchoolHoliday</th>\n","      <th>BeforeSchoolHoliday</th>\n","      <th>AfterStateHoliday</th>\n","      <th>BeforeStateHoliday</th>\n","      <th>AfterPromo</th>\n","      <th>BeforePromo</th>\n","      <th>SchoolHoliday_bw</th>\n","      <th>StateHoliday_bw</th>\n","      <th>Promo_bw</th>\n","      <th>SchoolHoliday_fw</th>\n","      <th>StateHoliday_fw</th>\n","      <th>Promo_fw</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2015-09-17</td>\n","      <td>1</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>105</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2015-09-16</td>\n","      <td>1</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2015-09-15</td>\n","      <td>1</td>\n","      <td>11</td>\n","      <td>0</td>\n","      <td>103</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2015-09-14</td>\n","      <td>1</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>102</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2015-09-13</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>101</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>-1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        Date  Store  AfterSchoolHoliday  BeforeSchoolHoliday  \\\n","0 2015-09-17      1                  13                    0   \n","1 2015-09-16      1                  12                    0   \n","2 2015-09-15      1                  11                    0   \n","3 2015-09-14      1                  10                    0   \n","4 2015-09-13      1                   9                    0   \n","\n","   AfterStateHoliday  BeforeStateHoliday  AfterPromo  BeforePromo  \\\n","0                105                   0           0            0   \n","1                104                   0           0            0   \n","2                103                   0           0            0   \n","3                102                   0           0            0   \n","4                101                   0           9           -1   \n","\n","   SchoolHoliday_bw  StateHoliday_bw  Promo_bw  SchoolHoliday_fw  \\\n","0               0.0              0.0       4.0               0.0   \n","1               0.0              0.0       3.0               0.0   \n","2               0.0              0.0       2.0               0.0   \n","3               0.0              0.0       1.0               0.0   \n","4               0.0              0.0       0.0               0.0   \n","\n","   StateHoliday_fw  Promo_fw  \n","0              0.0       1.0  \n","1              0.0       2.0  \n","2              0.0       3.0  \n","3              0.0       4.0  \n","4              0.0       4.0  "]},"metadata":{"tags":[]},"execution_count":0}]},{"metadata":{"id":"o8TTryFeCgzk","colab_type":"text"},"cell_type":"markdown","source":["It's usually a good idea to back up large tables of extracted / wrangled features before you join them onto another one, that way you can go back to it easily if you need to make changes to it."]},{"metadata":{"id":"lZYu8vu_Cgzk","colab_type":"text"},"cell_type":"markdown","source":["\n","通常は、抽出/まとめられた機能の大規模なテーブルを別の機能に結合する前にバックアップすることをお勧めします。変更する必要がある場合は、簡単に元に戻すことができます。 \n"]},{"metadata":{"id":"Env2x57NCgzl","colab_type":"code","colab":{}},"cell_type":"code","source":["df.to_pickle(PATH/'df')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L4fWEb2PCgzo","colab_type":"code","colab":{}},"cell_type":"code","source":["df[\"Date\"] = pd.to_datetime(df.Date)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wuiGJqPWCgzq","colab_type":"code","colab":{},"outputId":"b43eda74-fbae-4fa1-feba-3e32cbeae6d7"},"cell_type":"code","source":["df.columns"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Date', 'Store', 'AfterSchoolHoliday', 'BeforeSchoolHoliday',\n","       'AfterStateHoliday', 'BeforeStateHoliday', 'AfterPromo', 'BeforePromo',\n","       'SchoolHoliday_bw', 'StateHoliday_bw', 'Promo_bw', 'SchoolHoliday_fw',\n","       'StateHoliday_fw', 'Promo_fw'],\n","      dtype='object')"]},"metadata":{"tags":[]},"execution_count":0}]},{"metadata":{"id":"jUAXPuxZCgzt","colab_type":"code","colab":{}},"cell_type":"code","source":["joined = pd.read_pickle(PATH/'joined')\n","joined_test = pd.read_pickle(PATH/f'joined_test')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5uE_XdxhCgzv","colab_type":"code","colab":{}},"cell_type":"code","source":["joined = join_df(joined, df, ['Store', 'Date'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QKVy2gQWCgzy","colab_type":"code","colab":{}},"cell_type":"code","source":["joined_test = join_df(joined_test, df, ['Store', 'Date'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Waa8ikGECgz0","colab_type":"text"},"cell_type":"markdown","source":["The authors also removed all instances where the store had zero sale / was closed. We speculate that this may have cost them a higher standing in the competition. One reason this may be the case is that a little exploratory data analysis reveals that there are often periods where stores are closed, typically for refurbishment. Before and after these periods, there are naturally spikes in sales that one might expect. By ommitting this data from their training, the authors gave up the ability to leverage information about these periods to predict this otherwise volatile behavior."]},{"metadata":{"id":"QUBybk-xCgz0","colab_type":"text"},"cell_type":"markdown","source":["\n","著者はまた、店の販売数がゼロ/閉店済みのすべてのインスタンスを削除しました。これはコンペの中でより高い順位を犠牲にしたかもしれないと推測します。これが当てはまるかもしれない1つの理由は、ちょっとした探索的データ分析でわかることですが、店舗改装のために店が閉鎖される期間がしばしばあるようなのです。これらの期間の前後に、売上高の急上昇が当然予想されます。このデータを学習から除外することによって、著者らはこれらの期間についての情報を活用して、こうした不安定な行動を予測することを諦めました。\n"]},{"metadata":{"id":"wlH1sKphCgz1","colab_type":"code","colab":{}},"cell_type":"code","source":["joined = joined[joined.Sales!=0]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"M5jLLfPECgz3","colab_type":"text"},"cell_type":"markdown","source":["We'll back this up as well."]},{"metadata":{"id":"lvNI9e2qCgz4","colab_type":"text"},"cell_type":"markdown","source":["\n","これもバックアップします。 \n"]},{"metadata":{"id":"NSSS9BQGCgz5","colab_type":"code","colab":{}},"cell_type":"code","source":["joined.reset_index(inplace=True)\n","joined_test.reset_index(inplace=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ki5mabs5Cgz7","colab_type":"code","colab":{}},"cell_type":"code","source":["joined.to_pickle(PATH/'train_clean')\n","joined_test.to_pickle(PATH/'test_clean')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"btGw28DlCgz8","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}